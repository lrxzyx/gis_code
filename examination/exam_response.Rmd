
# Originality declaration  

I, [**Ruixuan Li**], confirm that the work presented in this assessment is my own. Where information has been derived from other sources, I confirm that this has been indicated in the work.

# Start your response here

```{r load libraries}
library(readr)
library(ggplot2)
library(sf)
library(dplyr)
library(spdep)
library(spatstat)
library(sp)
library(tmap)
library(tmaptools)
library(janitor)
library(tidyverse)
```

```{r data reading}
census_health <- read_csv("/Users/liruixuan/Desktop/UCL/GIS/examination/examination/census2021-ts037-lsoa.csv",na=c(" "))

boundary <- st_read("/Users/liruixuan/Desktop/UCL/GIS/examination/examination/Lower_layer_Super_Output_Areas_December_2021_Boundaries_EW_BFC_V10/Lower_layer_Super_Output_Areas_December_2021_Boundaries_EW_BFC_V10.shp")

```
data:
LSOA (Lower Layer Super Output Area) 2021
boundary data is too large to upload:
https://geoportal.statistics.gov.uk/datasets/2bbaef5230694f3abae4f9145a3a9800_0/explore  (shapefile) I removed all parentheses from the file name for easy reading of data

```{r}
#View the first few lines of data to understand the structure
head(census_health)
```


```{r Merge two data}
merged_data <- boundary %>%
  left_join(census_health, by = c("LSOA21CD" = "geography code"))
```

```{r}
colnames(merged_data)
```

```{r filter data}
#Filter out Manchester data
manchester_data <- merged_data %>%
  filter(grepl("Manchester", LSOA21NM, ignore.case = TRUE))
```

```{r select data}
#Filter out very healthy data
manchester_data_very <- manchester_data %>%
  select(-"General health: Good health",-"General health: Fair health",
         -"General health: Bad health",-"General health: Very bad health")
```



```{r tmap}
#Create a map
tm_shape(manchester_data_very) +
  tm_polygons("General health: Total: All usual residents", 
              title = "Health Status in Manchester") +
  tm_layout(main.title = "Very Health people in Manchester")
```

```{r}
window <- as.owin(manchester_data_very) 
plot(window)
```
```{r create a sp objec}
manchester_data_very_sp <- manchester_data_very %>%
  as(.,"Spatial")
```

```{r create a ppp object}
library(spatstat)

#Extract the centroid of a polygon as a point
centroids <- coordinates(manchester_data_very_sp)  #Extract centroid coordinates

#Create observation window (corrected to spatstat format)
bounding_box <- bbox(manchester_data_very_sp)  #Extract boundary range
window <- owin(xrange = bounding_box[1,], yrange = bounding_box[2,])  # 

#Create Point Pattern Object (PPP)
manchester_data_very_sp_ppp <- ppp(x = centroids[, 1], 
                              y = centroids[, 2], 
                              window = window)

#View point mode results
summary(manchester_data_very_sp_ppp)

#Visualization
plot(manchester_data_very_sp_ppp, main = "Point Pattern of Manchester Data")
```

```{r Ripleyâ€™s K}
K <- manchester_data_very_sp_ppp %>%
  Kest(.,correction="border") %>%
  plot()
```
The theoretical standard is the red straight line, which shows how the points should be spread out if they were randomly chosen.The steady black line shows the real pattern of observations and the real trend of point distribution;In the Manchester health data, the real points are more closely spaced than the random distribution, as shown by the fact that the black line is higher than the red line.Clustering is seen within the range of r.

```{r Kernel Density Estimation}
manchester_data_very_sp_ppp%>%
  density(., sigma=500) %>%
  plot()
```

Blue is a low-density area (with sparse point distribution). Yellow is a high-density area (with concentrated point distribution). The yellow areas are hotspots, indicating that health data is gathered in these locations, such as areas where self-reported health status may be more prominent. The blue areas with low density indicate that the distribution of health data in these areas is relatively small or scattered.

```{r Quadrat Analysis}
#First plot the points
plot(manchester_data_very_sp_ppp,
     pch=16,
     cex=0.5, 
     main="Blue Plaques in Harrow")

#now count the points in that fall in a 6 x 6
#grid overlaid across the windowBluePlaquesSub.ppp2<-BluePlaquesSub.ppp %>%
manchester_data_very_sp_ppp %>%
  quadratcount(.,nx = 6, ny = 6)%>%
    plot(., add=T, col="red")
```

```{r}
#run the quadrat count
Qcount <- manchester_data_very_sp_ppp %>%
  quadratcount(.,nx = 6, ny = 6) %>%
  as.data.frame() %>%
  dplyr::count(Var1=Freq)%>%
  dplyr::rename(Freqquadratcount=n)
Qcount %>% 
  summarise_all(class)
sums <- Qcount %>%
  #calculate the total blue plaques (Var * Freq)
  mutate(total = Var1 * Freqquadratcount) %>%
  dplyr::summarise(across(everything(), sum))%>%
  dplyr::select(-Var1) 

lambda<- Qcount%>%
  #calculate lambda
  mutate(total = Var1 * Freqquadratcount)%>%
  dplyr::summarise(across(everything(), sum)) %>%
  mutate(lambda=total/Freqquadratcount) %>%
  dplyr::select(lambda)%>%
  pull(lambda)
```

```{r}
QCountTable <- Qcount %>%
  mutate(Pr=((lambda^Var1)*exp(-lambda))/factorial(Var1))%>%
  #now calculate the expected counts based on our total number of plaques
  #and save them to the table
  mutate(Expected= (round(Pr * sums$Freqquadratcount, 0)))

#Compare the frequency distributions of the observed and expected point patterns
plot(c(1,5),c(0,14), type="n",
xlab="Number of Very Health people in Manchester (Red=Observed,Blue=Expected)", 
     ylab="Frequency of Occurances")
points(QCountTable$Freqquadratcount, 
       col="Red", 
       type="o", 
       lwd=3)
points(QCountTable$Expected, col="Blue", 
       type="o", 
       lwd=3)
```
Observed shows the real data that was collected, which shows how common the "very healthy" population is in Manchester across different groups. For instance, the number of observations in the first group is greater and slowly goes down after that. Blue (Expected) shows the frequency that was found using a statistical model or a theory of expectations. The expected value goes up slowly as the group changes, which is clearly not the same as the measured value shown in red.

```{r DBSCAN's clustering}
library(dbscan)
library(fpc)

#first extract the points from the spatial data frame
points_todf <- manchester_data_very_sp %>%
coordinates(.) %>%
as.data.frame()

#now run the dbscan analysis
points_todf_DBSCAN <- points_todf %>%
fpc::dbscan(.,eps=1000,MinPts=50) 

#Eps=1000: Set the maximum proximity distance between points (1000 meters).MinPts=50: Set the minimum number of points for each cluster (at least 50 points form a cluster).

points_todf %>%
dbscan::kNNdistplot(.,k=50)
#Calculate the distance between each point and its 50th nearest neighbor
 
plot(points_todf_DBSCAN,points_todf,main="DBSCAN output",frame=F)
plot(boundary$geometry,add=T)
```
The picture displays DBSCAN's clustering results, but it is currently disorganized and unable to tell the difference between groups. This suggests that either the parameters (like eps and minPts) have not been optimized or the data distribution is not right for DBSCAN. DBSCAN works well with data that has big differences between areas that are thin and areas that are dense. If the data is too dense or spread out too evenly, the result might not be very good.
```{r}
colnames(merged_data)
```


```{r}
#Convert to sf object
manchester_data_very_sf <- st_as_sf(manchester_data_very_sp)
```

```{r}
colnames(manchester_data_very_sf)
```


```{r}
points_sf <- manchester_data_very_sf %>%
  clean_names()%>%
  mutate(area=st_area(.)) %>%#Computes the area of each community (in units matching the CRS).
  mutate(density=general_health_very_good_health/general_health_total_all_usual_residents)#Calculates the eviction density by dividing the number of eviction points by the area of each community.
```

```{r}
tm_shape(points_sf)+
  tm_polygons("density",
              style="jenks",
              palette="PuOr",
              title="Eviction density")
```

```{r}

```




```{r}
library(spdep)
coordsw <-manchester_data_very_sf %>%
  st_centroid()%>% 
  #Computes the geometric centroids for each community area in manchester_data_very_sp.The centroid represents the center point of each polygon.
  st_geometry() #Extracts the geometric information (coordinates) of the centroids.
plot(coordsw,axes=TRUE)

#make neighbours

community_nb <- manchester_data_very_sf %>%
  poly2nb(.,queen=T) #Generates an adjacency list for the polygons, describing which community areas are neighbors.
summary(community_nb)
#Plots the adjacency relationships on the map.
#Red lines connect centroids of adjacent community areas.
plot(community_nb,st_geometry(coordsw),col="red")
#Overlays the actual community boundaries (polygons) on the same map to provide spatial context.
plot(manchester_data_very_sp$geometry,add=T)
  
```

```{r}
#make weight matrix
community_nb.lw <- community_nb %>%
  nb2mat(.,style ="W")

sum(community_nb.lw)
```
This means there are 295 neighbor relationships between communities.


```{r}
#Converts the community adjacency list (community_nb) into a spatial weight matrix.
#style = "W": Creates a row-standardized weight matrix where the sum of each row equals 1.
community_nb.lw <- nb2listw(community_nb, zero.policy = TRUE)
```

```{r}
I_LWard_Global_Density <- manchester_data_very_sf %>%
  pull(density)%>%
  as.vector()%>%
  localmoran(.,community_nb.lw)#Computes Local Moran's I index for each community to analyze local spatial autocorrelation.
I_LWard_Global_Density
```

```{r}

```

## Initial project scope